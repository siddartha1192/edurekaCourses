{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siddartha1192/edurekaCourses/blob/main/NLP_demo_Aug_25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "454brUa3ST5V"
      },
      "outputs": [],
      "source": [
        "#import NLTK\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize,word_tokenize\n",
        "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JaQrIYmUJIl",
        "outputId": "76709b4f-77f4-4e77-c828-e26b81f22ee0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Sample Text\n",
        "text=\"\"\"Artificial Intelligence (AI) is transforming the\n",
        "way businesses operate across the globe.\n",
        "From healthcare to finance, AI-powered systems are helping organizations improve efficiency,\n",
        "reduce costs, and make smarter decisions.\n",
        "For example, chatbots are providing customer support 24/7,\n",
        "while recommendation systems are personalizing shopping experiences for millions of users.\n",
        "Despite these benefits, experts warn about ethical concerns, such as data privacy and job displacement.\n",
        "Therefore, it is essential to balance innovation with responsibility when adopting AI technologies.\"\"\""
      ],
      "metadata": {
        "id": "WIPqZewqVVLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sentence Tokenizer\n",
        "sentences=sent_tokenize(text)\n",
        "print(sentences)\n",
        "\n",
        "for sen in sentences:\n",
        "    print(sen)\n",
        "\n",
        "print(len(sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czyJg6GSV9zM",
        "outputId": "ae3aaab1-0779-4b26-bb4b-becf817201ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Artificial Intelligence (AI) is transforming the\\nway businesses operate across the globe.', 'From healthcare to finance, AI-powered systems are helping organizations improve efficiency,\\nreduce costs, and make smarter decisions.', 'For example, chatbots are providing customer support 24/7,\\nwhile recommendation systems are personalizing shopping experiences for millions of users.', 'Despite these benefits, experts warn about ethical concerns, such as data privacy and job displacement.', 'Therefore, it is essential to balance innovation with responsibility when adopting AI technologies.']\n",
            "Artificial Intelligence (AI) is transforming the\n",
            "way businesses operate across the globe.\n",
            "From healthcare to finance, AI-powered systems are helping organizations improve efficiency,\n",
            "reduce costs, and make smarter decisions.\n",
            "For example, chatbots are providing customer support 24/7,\n",
            "while recommendation systems are personalizing shopping experiences for millions of users.\n",
            "Despite these benefits, experts warn about ethical concerns, such as data privacy and job displacement.\n",
            "Therefore, it is essential to balance innovation with responsibility when adopting AI technologies.\n",
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Word Tokenize\n",
        "words=word_tokenize(text)\n",
        "print(words)\n",
        "print(len(words))\n",
        "\n",
        "for word in words:\n",
        "    print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9A8RRZiXEd3",
        "outputId": "ddf17498-1895-4c49-ca9b-d4f45fa8db30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Artificial', 'Intelligence', '(', 'AI', ')', 'is', 'transforming', 'the', 'way', 'businesses', 'operate', 'across', 'the', 'globe', '.', 'From', 'healthcare', 'to', 'finance', ',', 'AI-powered', 'systems', 'are', 'helping', 'organizations', 'improve', 'efficiency', ',', 'reduce', 'costs', ',', 'and', 'make', 'smarter', 'decisions', '.', 'For', 'example', ',', 'chatbots', 'are', 'providing', 'customer', 'support', '24/7', ',', 'while', 'recommendation', 'systems', 'are', 'personalizing', 'shopping', 'experiences', 'for', 'millions', 'of', 'users', '.', 'Despite', 'these', 'benefits', ',', 'experts', 'warn', 'about', 'ethical', 'concerns', ',', 'such', 'as', 'data', 'privacy', 'and', 'job', 'displacement', '.', 'Therefore', ',', 'it', 'is', 'essential', 'to', 'balance', 'innovation', 'with', 'responsibility', 'when', 'adopting', 'AI', 'technologies', '.']\n",
            "91\n",
            "Artificial\n",
            "Intelligence\n",
            "(\n",
            "AI\n",
            ")\n",
            "is\n",
            "transforming\n",
            "the\n",
            "way\n",
            "businesses\n",
            "operate\n",
            "across\n",
            "the\n",
            "globe\n",
            ".\n",
            "From\n",
            "healthcare\n",
            "to\n",
            "finance\n",
            ",\n",
            "AI-powered\n",
            "systems\n",
            "are\n",
            "helping\n",
            "organizations\n",
            "improve\n",
            "efficiency\n",
            ",\n",
            "reduce\n",
            "costs\n",
            ",\n",
            "and\n",
            "make\n",
            "smarter\n",
            "decisions\n",
            ".\n",
            "For\n",
            "example\n",
            ",\n",
            "chatbots\n",
            "are\n",
            "providing\n",
            "customer\n",
            "support\n",
            "24/7\n",
            ",\n",
            "while\n",
            "recommendation\n",
            "systems\n",
            "are\n",
            "personalizing\n",
            "shopping\n",
            "experiences\n",
            "for\n",
            "millions\n",
            "of\n",
            "users\n",
            ".\n",
            "Despite\n",
            "these\n",
            "benefits\n",
            ",\n",
            "experts\n",
            "warn\n",
            "about\n",
            "ethical\n",
            "concerns\n",
            ",\n",
            "such\n",
            "as\n",
            "data\n",
            "privacy\n",
            "and\n",
            "job\n",
            "displacement\n",
            ".\n",
            "Therefore\n",
            ",\n",
            "it\n",
            "is\n",
            "essential\n",
            "to\n",
            "balance\n",
            "innovation\n",
            "with\n",
            "responsibility\n",
            "when\n",
            "adopting\n",
            "AI\n",
            "technologies\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Stop word Removal\n",
        "stop_words=set(stopwords.words('english'))\n",
        "print(stop_words)\n",
        "filtered_words=[word for word in words if word.lower() not in stop_words]  #word.casefold() will ignore the case\n",
        "print(filtered_words)\n",
        "print(len(filtered_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PWtcZLjXTfd",
        "outputId": "227d5e4d-8252-4069-d544-15e2455c0599"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'than', 'your', 'himself', 'isn', \"he'll\", \"wouldn't\", 'against', 'o', 'd', \"they're\", 'of', 'wasn', 'other', 'does', 'me', 'each', 'am', 'then', \"you'll\", 'above', \"we've\", 'from', \"she'd\", 'about', 'any', \"she's\", \"she'll\", 'm', \"i'd\", 't', 'weren', 'the', 'so', 'after', 'having', 'under', 'hadn', 'when', 'ourselves', 'y', 'by', 'him', \"it'll\", 'too', \"it's\", 'off', 'they', 'not', 'doing', 'didn', 'below', 'our', \"you're\", 'this', 'hasn', 'theirs', 'both', 'we', 'needn', 'itself', 'until', 'was', 'how', \"should've\", 'there', \"hadn't\", 'it', 'll', 'are', \"don't\", 'ours', 'myself', 're', 'had', 'these', \"i'm\", \"we'd\", 's', 'if', 'their', 'which', 'he', 'during', 'can', \"needn't\", \"aren't\", 'down', \"it'd\", \"mightn't\", 'between', 'shouldn', 'as', 'further', 'its', 'those', 'an', 'ma', \"we'll\", 'her', 'all', 'very', 'been', 'and', 'do', 'out', 'while', 'aren', 'again', \"weren't\", \"he'd\", 'mustn', \"we're\", 'wouldn', \"shan't\", 'no', 'or', \"mustn't\", 'shan', 'won', 'yourselves', 'be', 'for', \"didn't\", \"they'd\", 'herself', \"they've\", 'did', 'own', 'has', 'is', 'in', 'you', 'now', 'she', 'my', \"he's\", 'before', \"hasn't\", \"that'll\", 'to', \"you've\", 'some', 'them', 'yours', 'being', 'that', \"couldn't\", 'themselves', 'such', 'i', 'where', 'why', 'mightn', 'his', 'will', 'over', \"i'll\", 'same', \"wasn't\", 'into', 'don', \"i've\", 'hers', \"isn't\", 'once', 'haven', 'yourself', 'because', 'more', 'whom', 'with', 'through', 'couldn', \"you'd\", 'have', 'few', 'ain', 'a', 'only', 've', 'who', 'nor', 'most', 'should', 'at', 'up', 'just', 'doesn', \"they'll\", \"doesn't\", 'were', 'what', 'but', \"shouldn't\", 'on', \"won't\", 'here', \"haven't\"}\n",
            "['Artificial', 'Intelligence', '(', 'AI', ')', 'transforming', 'way', 'businesses', 'operate', 'across', 'globe', '.', 'healthcare', 'finance', ',', 'AI-powered', 'systems', 'helping', 'organizations', 'improve', 'efficiency', ',', 'reduce', 'costs', ',', 'make', 'smarter', 'decisions', '.', 'example', ',', 'chatbots', 'providing', 'customer', 'support', '24/7', ',', 'recommendation', 'systems', 'personalizing', 'shopping', 'experiences', 'millions', 'users', '.', 'Despite', 'benefits', ',', 'experts', 'warn', 'ethical', 'concerns', ',', 'data', 'privacy', 'job', 'displacement', '.', 'Therefore', ',', 'essential', 'balance', 'innovation', 'responsibility', 'adopting', 'AI', 'technologies', '.']\n",
            "68\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming **"
      ],
      "metadata": {
        "id": "YrtyzF1DBcZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
        "\n",
        "ps=PorterStemmer()\n",
        "stemmed_words=[ps.stem(word) for word in filtered_words]\n",
        "print(stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tG4oB_-BfGL",
        "outputId": "50724a20-75d1-4ea0-c299-3a54607c6c89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['artifici', 'intellig', '(', 'ai', ')', 'transform', 'way', 'busi', 'oper', 'across', 'globe', '.', 'healthcar', 'financ', ',', 'ai-pow', 'system', 'help', 'organ', 'improv', 'effici', ',', 'reduc', 'cost', ',', 'make', 'smarter', 'decis', '.', 'exampl', ',', 'chatbot', 'provid', 'custom', 'support', '24/7', ',', 'recommend', 'system', 'person', 'shop', 'experi', 'million', 'user', '.', 'despit', 'benefit', ',', 'expert', 'warn', 'ethic', 'concern', ',', 'data', 'privaci', 'job', 'displac', '.', 'therefor', ',', 'essenti', 'balanc', 'innov', 'respons', 'adopt', 'ai', 'technolog', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for words in filtered_words:\n",
        "  print(words+\":\"+ps.stem(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhCIAa9OCLck",
        "outputId": "ee059290-8da7-479a-fc09-eb8bdeb91e35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artificial:artifici\n",
            "Intelligence:intellig\n",
            "(:(\n",
            "AI:ai\n",
            "):)\n",
            "transforming:transform\n",
            "way:way\n",
            "businesses:busi\n",
            "operate:oper\n",
            "across:across\n",
            "globe:globe\n",
            ".:.\n",
            "healthcare:healthcar\n",
            "finance:financ\n",
            ",:,\n",
            "AI-powered:ai-pow\n",
            "systems:system\n",
            "helping:help\n",
            "organizations:organ\n",
            "improve:improv\n",
            "efficiency:effici\n",
            ",:,\n",
            "reduce:reduc\n",
            "costs:cost\n",
            ",:,\n",
            "make:make\n",
            "smarter:smarter\n",
            "decisions:decis\n",
            ".:.\n",
            "example:exampl\n",
            ",:,\n",
            "chatbots:chatbot\n",
            "providing:provid\n",
            "customer:custom\n",
            "support:support\n",
            "24/7:24/7\n",
            ",:,\n",
            "recommendation:recommend\n",
            "systems:system\n",
            "personalizing:person\n",
            "shopping:shop\n",
            "experiences:experi\n",
            "millions:million\n",
            "users:user\n",
            ".:.\n",
            "Despite:despit\n",
            "benefits:benefit\n",
            ",:,\n",
            "experts:expert\n",
            "warn:warn\n",
            "ethical:ethic\n",
            "concerns:concern\n",
            ",:,\n",
            "data:data\n",
            "privacy:privaci\n",
            "job:job\n",
            "displacement:displac\n",
            ".:.\n",
            "Therefore:therefor\n",
            ",:,\n",
            "essential:essenti\n",
            "balance:balanc\n",
            "innovation:innov\n",
            "responsibility:respons\n",
            "adopting:adopt\n",
            "AI:ai\n",
            "technologies:technolog\n",
            ".:.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lancaster Stemmer\n",
        "ls=LancasterStemmer()\n",
        "stemmed_words=[ls.stem(word) for word in filtered_words]\n",
        "print(stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAN2x2PbCrDj",
        "outputId": "297b58c7-c9f3-420d-a6d5-0e9d0b580b13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['art', 'intellig', '(', 'ai', ')', 'transform', 'way', 'busy', 'op', 'across', 'glob', '.', 'healthc', 'fin', ',', 'ai-powered', 'system', 'help', 'org', 'improv', 'efficy', ',', 'reduc', 'cost', ',', 'mak', 'smart', 'decid', '.', 'exampl', ',', 'chatbot', 'provid', 'custom', 'support', '24/7', ',', 'recommend', 'system', 'person', 'shop', 'expery', 'mil', 'us', '.', 'despit', 'benefit', ',', 'expert', 'warn', 'eth', 'concern', ',', 'dat', 'priv', 'job', 'displac', '.', 'theref', ',', 'ess', 'bal', 'innov', 'respons', 'adopt', 'ai', 'technolog', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for words in filtered_words:\n",
        "  print(words+\":\"+ls.stem(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1Dud2F1C_Td",
        "outputId": "7a7ab159-4b19-4964-e6bc-8cb57f7a17b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artificial:art\n",
            "Intelligence:intellig\n",
            "(:(\n",
            "AI:ai\n",
            "):)\n",
            "transforming:transform\n",
            "way:way\n",
            "businesses:busy\n",
            "operate:op\n",
            "across:across\n",
            "globe:glob\n",
            ".:.\n",
            "healthcare:healthc\n",
            "finance:fin\n",
            ",:,\n",
            "AI-powered:ai-powered\n",
            "systems:system\n",
            "helping:help\n",
            "organizations:org\n",
            "improve:improv\n",
            "efficiency:efficy\n",
            ",:,\n",
            "reduce:reduc\n",
            "costs:cost\n",
            ",:,\n",
            "make:mak\n",
            "smarter:smart\n",
            "decisions:decid\n",
            ".:.\n",
            "example:exampl\n",
            ",:,\n",
            "chatbots:chatbot\n",
            "providing:provid\n",
            "customer:custom\n",
            "support:support\n",
            "24/7:24/7\n",
            ",:,\n",
            "recommendation:recommend\n",
            "systems:system\n",
            "personalizing:person\n",
            "shopping:shop\n",
            "experiences:expery\n",
            "millions:mil\n",
            "users:us\n",
            ".:.\n",
            "Despite:despit\n",
            "benefits:benefit\n",
            ",:,\n",
            "experts:expert\n",
            "warn:warn\n",
            "ethical:eth\n",
            "concerns:concern\n",
            ",:,\n",
            "data:dat\n",
            "privacy:priv\n",
            "job:job\n",
            "displacement:displac\n",
            ".:.\n",
            "Therefore:theref\n",
            ",:,\n",
            "essential:ess\n",
            "balance:bal\n",
            "innovation:innov\n",
            "responsibility:respons\n",
            "adopting:adopt\n",
            "AI:ai\n",
            "technologies:technolog\n",
            ".:.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Snow Ball Stemmer: for multiple languages\n",
        "print(\" \".join(SnowballStemmer.languages))\n",
        "\n",
        "sample=\"Les √©tudiants aiment √©tudier les diff√©rentes √©tudes.\"\n",
        "\n",
        "sample=word_tokenize(sample)\n",
        "print(sample)\n",
        "\n",
        "ss=SnowballStemmer('french')\n",
        "stemmed_words=[ss.stem(word) for word in sample]\n",
        "print(stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_68-vrl2D3eL",
        "outputId": "2685900a-dc79-4f33-f5c7-7888abde6c80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "arabic danish dutch english finnish french german hungarian italian norwegian porter portuguese romanian russian spanish swedish\n",
            "['Les', '√©tudiants', 'aiment', '√©tudier', 'les', 'diff√©rentes', '√©tudes', '.']\n",
            "['le', '√©tudi', 'aiment', '√©tudi', 'le', 'diff√©rent', '√©tud', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ss=SnowballStemmer('english')\n",
        "stemmed_words=[ss.stem(word) for word in filtered_words]\n",
        "print(stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZwXsWm9FXKn",
        "outputId": "f81a23be-2739-47f0-ee47-2f4e307933cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['artifici', 'intellig', '(', 'ai', ')', 'transform', 'way', 'busi', 'oper', 'across', 'globe', '.', 'healthcar', 'financ', ',', 'ai-pow', 'system', 'help', 'organ', 'improv', 'effici', ',', 'reduc', 'cost', ',', 'make', 'smarter', 'decis', '.', 'exampl', ',', 'chatbot', 'provid', 'custom', 'support', '24/7', ',', 'recommend', 'system', 'person', 'shop', 'experi', 'million', 'user', '.', 'despit', 'benefit', ',', 'expert', 'warn', 'ethic', 'concern', ',', 'data', 'privaci', 'job', 'displac', '.', 'therefor', ',', 'essenti', 'balanc', 'innov', 'respons', 'adopt', 'ai', 'technolog', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for words in filtered_words:\n",
        "  print(words+\":\"+ss.stem(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmxMAkGeFb1g",
        "outputId": "8e7b926f-fc59-4ea3-e094-9af7232ae8cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artificial:artifici\n",
            "Intelligence:intellig\n",
            "(:(\n",
            "AI:ai\n",
            "):)\n",
            "transforming:transform\n",
            "way:way\n",
            "businesses:busi\n",
            "operate:oper\n",
            "across:across\n",
            "globe:globe\n",
            ".:.\n",
            "healthcare:healthcar\n",
            "finance:financ\n",
            ",:,\n",
            "AI-powered:ai-pow\n",
            "systems:system\n",
            "helping:help\n",
            "organizations:organ\n",
            "improve:improv\n",
            "efficiency:effici\n",
            ",:,\n",
            "reduce:reduc\n",
            "costs:cost\n",
            ",:,\n",
            "make:make\n",
            "smarter:smarter\n",
            "decisions:decis\n",
            ".:.\n",
            "example:exampl\n",
            ",:,\n",
            "chatbots:chatbot\n",
            "providing:provid\n",
            "customer:custom\n",
            "support:support\n",
            "24/7:24/7\n",
            ",:,\n",
            "recommendation:recommend\n",
            "systems:system\n",
            "personalizing:person\n",
            "shopping:shop\n",
            "experiences:experi\n",
            "millions:million\n",
            "users:user\n",
            ".:.\n",
            "Despite:despit\n",
            "benefits:benefit\n",
            ",:,\n",
            "experts:expert\n",
            "warn:warn\n",
            "ethical:ethic\n",
            "concerns:concern\n",
            ",:,\n",
            "data:data\n",
            "privacy:privaci\n",
            "job:job\n",
            "displacement:displac\n",
            ".:.\n",
            "Therefore:therefor\n",
            ",:,\n",
            "essential:essenti\n",
            "balance:balanc\n",
            "innovation:innov\n",
            "responsibility:respons\n",
            "adopting:adopt\n",
            "AI:ai\n",
            "technologies:technolog\n",
            ".:.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization : WordNetLemmatizer"
      ],
      "metadata": {
        "id": "nTfJvnDqN-kw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "ps=PorterStemmer()\n",
        "stemmed_word=ps.stem(\"Study\")\n",
        "lemmatized_word=lemmatizer.lemmatize(\"Study\")\n",
        "print(\"Stemmed output:\",stemmed_word)\n",
        "print(\"Lemma output:\",lemmatized_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uA86QDLPN2tB",
        "outputId": "bf3055ed-fe70-4442-949f-041e4f395e29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed output: studi\n",
            "Lemma output: Study\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatized_words=[lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "stemmed_words=[ps.stem(word) for word in filtered_words]\n",
        "\n",
        "for i in range(len(filtered_words)):\n",
        "  print(filtered_words[i]+\" Lemma:\"+lemmatized_words[i]+\" Stem:\"+stemmed_words[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWJtFFGAPRY3",
        "outputId": "77421a88-9083-4b6c-eb22-305a7e0e8f57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artificial Lemma:Artificial Stem:artifici\n",
            "Intelligence Lemma:Intelligence Stem:intellig\n",
            "( Lemma:( Stem:(\n",
            "AI Lemma:AI Stem:ai\n",
            ") Lemma:) Stem:)\n",
            "transforming Lemma:transforming Stem:transform\n",
            "way Lemma:way Stem:way\n",
            "businesses Lemma:business Stem:busi\n",
            "operate Lemma:operate Stem:oper\n",
            "across Lemma:across Stem:across\n",
            "globe Lemma:globe Stem:globe\n",
            ". Lemma:. Stem:.\n",
            "healthcare Lemma:healthcare Stem:healthcar\n",
            "finance Lemma:finance Stem:financ\n",
            ", Lemma:, Stem:,\n",
            "AI-powered Lemma:AI-powered Stem:ai-pow\n",
            "systems Lemma:system Stem:system\n",
            "helping Lemma:helping Stem:help\n",
            "organizations Lemma:organization Stem:organ\n",
            "improve Lemma:improve Stem:improv\n",
            "efficiency Lemma:efficiency Stem:effici\n",
            ", Lemma:, Stem:,\n",
            "reduce Lemma:reduce Stem:reduc\n",
            "costs Lemma:cost Stem:cost\n",
            ", Lemma:, Stem:,\n",
            "make Lemma:make Stem:make\n",
            "smarter Lemma:smarter Stem:smarter\n",
            "decisions Lemma:decision Stem:decis\n",
            ". Lemma:. Stem:.\n",
            "example Lemma:example Stem:exampl\n",
            ", Lemma:, Stem:,\n",
            "chatbots Lemma:chatbots Stem:chatbot\n",
            "providing Lemma:providing Stem:provid\n",
            "customer Lemma:customer Stem:custom\n",
            "support Lemma:support Stem:support\n",
            "24/7 Lemma:24/7 Stem:24/7\n",
            ", Lemma:, Stem:,\n",
            "recommendation Lemma:recommendation Stem:recommend\n",
            "systems Lemma:system Stem:system\n",
            "personalizing Lemma:personalizing Stem:person\n",
            "shopping Lemma:shopping Stem:shop\n",
            "experiences Lemma:experience Stem:experi\n",
            "millions Lemma:million Stem:million\n",
            "users Lemma:user Stem:user\n",
            ". Lemma:. Stem:.\n",
            "Despite Lemma:Despite Stem:despit\n",
            "benefits Lemma:benefit Stem:benefit\n",
            ", Lemma:, Stem:,\n",
            "experts Lemma:expert Stem:expert\n",
            "warn Lemma:warn Stem:warn\n",
            "ethical Lemma:ethical Stem:ethic\n",
            "concerns Lemma:concern Stem:concern\n",
            ", Lemma:, Stem:,\n",
            "data Lemma:data Stem:data\n",
            "privacy Lemma:privacy Stem:privaci\n",
            "job Lemma:job Stem:job\n",
            "displacement Lemma:displacement Stem:displac\n",
            ". Lemma:. Stem:.\n",
            "Therefore Lemma:Therefore Stem:therefor\n",
            ", Lemma:, Stem:,\n",
            "essential Lemma:essential Stem:essenti\n",
            "balance Lemma:balance Stem:balanc\n",
            "innovation Lemma:innovation Stem:innov\n",
            "responsibility Lemma:responsibility Stem:respons\n",
            "adopting Lemma:adopting Stem:adopt\n",
            "AI Lemma:AI Stem:ai\n",
            "technologies Lemma:technology Stem:technolog\n",
            ". Lemma:. Stem:.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Handling Special Characters **"
      ],
      "metadata": {
        "id": "BxmCiFzxRFqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "special_chars=set(string.punctuation)\n",
        "print(special_chars)\n",
        "\n",
        "lst=[\":)\",\":(\"]\n",
        "special_chars.update(lst)\n",
        "print(special_chars)\n",
        "\n",
        "filtered_words=[word for word in filtered_words if word not in list(special_chars)]\n",
        "print(filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJKxzPdORltt",
        "outputId": "4b4a64d6-b9fd-46a0-d962-f4abaa72f146"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'-', '{', '=', '<', '_', '*', '>', '.', ';', '?', ',', '%', ']', '!', '\\\\', '@', '&', ':', '+', '\"', '|', '/', '}', '[', '~', ')', '`', '(', '$', '#', \"'\", '^'}\n",
            "['Artificial', 'Intelligence', 'AI', 'transforming', 'way', 'businesses', 'operate', 'across', 'globe', 'healthcare', 'finance', 'AI-powered', 'systems', 'helping', 'organizations', 'improve', 'efficiency', 'reduce', 'costs', 'make', 'smarter', 'decisions', 'example', 'chatbots', 'providing', 'customer', 'support', '24/7', 'recommendation', 'systems', 'personalizing', 'shopping', 'experiences', 'millions', 'users', 'Despite', 'benefits', 'experts', 'warn', 'ethical', 'concerns', 'data', 'privacy', 'job', 'displacement', 'Therefore', 'essential', 'balance', 'innovation', 'responsibility', 'adopting', 'AI', 'technologies']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting in into Lemmas\n",
        "filtered_words=[lemmatizer.lemmatize(word) for word in filtered_words]"
      ],
      "metadata": {
        "id": "YZnuFD7ZSajh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in filtered_words:\n",
        "  print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43tzf_BkTzLN",
        "outputId": "002e54fc-8521-4bdc-9cf3-3695b65ce882"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artificial\n",
            "Intelligence\n",
            "AI\n",
            "transforming\n",
            "way\n",
            "business\n",
            "operate\n",
            "across\n",
            "globe\n",
            "healthcare\n",
            "finance\n",
            "AI-powered\n",
            "system\n",
            "helping\n",
            "organization\n",
            "improve\n",
            "efficiency\n",
            "reduce\n",
            "cost\n",
            "make\n",
            "smarter\n",
            "decision\n",
            "example\n",
            "chatbots\n",
            "providing\n",
            "customer\n",
            "support\n",
            "24/7\n",
            "recommendation\n",
            "system\n",
            "personalizing\n",
            "shopping\n",
            "experience\n",
            "million\n",
            "user\n",
            "Despite\n",
            "benefit\n",
            "expert\n",
            "warn\n",
            "ethical\n",
            "concern\n",
            "data\n",
            "privacy\n",
            "job\n",
            "displacement\n",
            "Therefore\n",
            "essential\n",
            "balance\n",
            "innovation\n",
            "responsibility\n",
            "adopting\n",
            "AI\n",
            "technology\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part of Speech Tagging**"
      ],
      "metadata": {
        "id": "MMz1KTeFUFfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Resource for POS tagging for English Language\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZc3dW4-UKIx",
        "outputId": "61da2213-7444-4c9e-8340-54ce1a41ac25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_with_pos=nltk.pos_tag(filtered_words)\n",
        "\n",
        "for token, pos_tag in tokens_with_pos:\n",
        "  print(f\"{token}:{pos_tag}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxTFt4S1V0Df",
        "outputId": "d0f90f7a-0b07-4a19-a386-2c29fc42ddbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artificial:JJ\n",
            "Intelligence:NNP\n",
            "AI:NNP\n",
            "transforming:VBG\n",
            "way:NN\n",
            "business:NN\n",
            "operate:VBP\n",
            "across:IN\n",
            "globe:NN\n",
            "healthcare:NN\n",
            "finance:NN\n",
            "AI-powered:NNP\n",
            "system:NN\n",
            "helping:VBG\n",
            "organization:NN\n",
            "improve:VB\n",
            "efficiency:NN\n",
            "reduce:VB\n",
            "cost:NN\n",
            "make:VBP\n",
            "smarter:JJR\n",
            "decision:NN\n",
            "example:NN\n",
            "chatbots:NNS\n",
            "providing:VBG\n",
            "customer:NN\n",
            "support:NN\n",
            "24/7:CD\n",
            "recommendation:NN\n",
            "system:NN\n",
            "personalizing:VBG\n",
            "shopping:VBG\n",
            "experience:NN\n",
            "million:CD\n",
            "user:IN\n",
            "Despite:IN\n",
            "benefit:NN\n",
            "expert:NN\n",
            "warn:VBP\n",
            "ethical:JJ\n",
            "concern:NN\n",
            "data:NNS\n",
            "privacy:NN\n",
            "job:NN\n",
            "displacement:NN\n",
            "Therefore:NNP\n",
            "essential:JJ\n",
            "balance:NN\n",
            "innovation:NN\n",
            "responsibility:NN\n",
            "adopting:VBG\n",
            "AI:NNP\n",
            "technology:NN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CC coordinating conjunction\n",
        "CD cardinal digit\n",
        "DT determiner\n",
        "EX existential there (like: \"there is\" ... think of it like \"there exists\")\n",
        "FW foreign word\n",
        "IN preposition/subordinating conjunction\n",
        "JJ adjective - 'big'\n",
        "JJR adjective, comparative - 'bigger'\n",
        "JJS adjective, superlative - 'biggest'\n",
        "LS list marker 1)\n",
        "MD modal - could, will\n",
        "NN noun, singular '- desk'\n",
        "NNS noun plural - 'desks'\n",
        "NNP proper noun, singular - 'Harrison'\n",
        "NNPS proper noun, plural - 'Americans'\n",
        "PDT predeterminer - 'all the kids'\n",
        "POS possessive ending parent's\n",
        "PRP personal pronoun -  I, he, she\n",
        "PRP$ possessive pronoun - my, his, hers\n",
        "RB adverb - very, silently,\n",
        "RBR adverb, comparative - better\n",
        "RBS adverb, superlative - best\n",
        "RP particle - give up\n",
        "TO - to go 'to' the store.\n",
        "UH interjection - errrrrrrrm\n",
        "VB verb, base form - take\n",
        "VBD verb, past tense - took\n",
        "VBG verb, gerund/present participle - taking\n",
        "VBN verb, past participle - taken\n",
        "VBP verb, sing. present, non-3d - take\n",
        "VBZ verb, 3rd person sing. present - takes\n",
        "WDT wh-determiner - which\n",
        "WP wh-pronoun - who, what\n",
        "WP$ possessive wh-pronoun, eg- whose\n",
        "WRB wh-adverb, eg- where, when"
      ],
      "metadata": {
        "id": "ynakJO6JWnmy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handling Messy Data, full of Emojis,Hash,URLs,@"
      ],
      "metadata": {
        "id": "Zs_z39yX3hwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv(\"/content/multilingual_twitter_dataset.csv\")\n",
        "df.head(50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8nJ9g8UO3ges",
        "outputId": "7fe3b986-78ff-42f5-96c1-ecfa4ef19e96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   username   location  gender  age  \\\n",
              "0    user_1      India  Female   43   \n",
              "1    user_2     France    Male   26   \n",
              "2    user_3    Germany   Other   30   \n",
              "3    user_4    Germany  Female   38   \n",
              "4    user_5      Japan  Female   39   \n",
              "5    user_6        USA    Male   45   \n",
              "6    user_7     France   Other   24   \n",
              "7    user_8     Brazil  Female   47   \n",
              "8    user_9      Egypt   Other   36   \n",
              "9   user_10        USA    Male   54   \n",
              "10  user_11      Spain  Female   38   \n",
              "11  user_12     France   Other   31   \n",
              "12  user_13    Germany    Male   19   \n",
              "13  user_14     France  Female   57   \n",
              "14  user_15    Germany   Other   34   \n",
              "15  user_16      Japan    Male   22   \n",
              "16  user_17  Indonesia    Male   44   \n",
              "17  user_18     Brazil  Female   40   \n",
              "18  user_19    Germany  Female   18   \n",
              "19  user_20      Egypt   Other   34   \n",
              "20  user_21     France    Male   28   \n",
              "21  user_22     Brazil   Other   27   \n",
              "22  user_23      Egypt   Other   43   \n",
              "23  user_24     France   Other   34   \n",
              "24  user_25        USA    Male   46   \n",
              "25  user_26    Germany    Male   23   \n",
              "26  user_27        USA   Other   35   \n",
              "27  user_28      Egypt   Other   30   \n",
              "28  user_29        USA   Other   31   \n",
              "29  user_30     Russia  Female   56   \n",
              "30  user_31  Indonesia   Other   49   \n",
              "31  user_32      Japan  Female   48   \n",
              "32  user_33      Japan   Other   54   \n",
              "33  user_34     Brazil    Male   39   \n",
              "34  user_35  Indonesia   Other   19   \n",
              "35  user_36      India   Other   19   \n",
              "36  user_37    Germany    Male   30   \n",
              "37  user_38     Brazil  Female   33   \n",
              "38  user_39        USA    Male   47   \n",
              "39  user_40      Japan    Male   18   \n",
              "40  user_41      Japan    Male   47   \n",
              "41  user_42      Spain    Male   29   \n",
              "42  user_43    Germany  Female   29   \n",
              "43  user_44      Japan   Other   53   \n",
              "44  user_45        USA  Female   55   \n",
              "45  user_46        USA    Male   42   \n",
              "46  user_47     France   Other   30   \n",
              "47  user_48  Indonesia    Male   18   \n",
              "48  user_49      Japan    Male   30   \n",
              "49  user_50  Indonesia  Female   29   \n",
              "\n",
              "                                            tweet  \n",
              "0   Can't believe this happened... lol :D #fail üòÖ  \n",
              "1                                      ‰ø°„Åò„Çâ„Çå„Å™„ÅÑ‚Ä¶ üò≠üíî  \n",
              "2                  Das ist fantastisch üòçüíØ! #Liebe  \n",
              "3   Can't believe this happened... lol :D #fail üòÖ  \n",
              "4                   ¬°Esto es perfecto! üòäüíÉ #fiesta  \n",
              "5                           –≠—Ç–æ –ø—Ä–æ—Å—Ç–æ —Å—É–ø–µ—Ä! üî•üî•üî•  \n",
              "6                 Ich kann das nicht glauben!! üò†üò§  \n",
              "7                Je n‚Äôaime pas √ßa... :( #triste üò¢  \n",
              "8                      Ini luar biasa! üòçüëè #senang  \n",
              "9                Je n‚Äôaime pas √ßa... :( #triste üò¢  \n",
              "10                 Das ist fantastisch üòçüíØ! #Liebe  \n",
              "11        C‚Äôest incroyable üò≤! Tellement content üíñ  \n",
              "12                 Isso √© demais cara! üòé‚úåÔ∏è #feliz  \n",
              "13                 Das ist fantastisch üòçüíØ! #Liebe  \n",
              "14                                   ÿ±ÿßÿ¶ÿπ ÿ¨ÿØÿß üòÇüî•üëç  \n",
              "15                                     ‰ø°„Åò„Çâ„Çå„Å™„ÅÑ‚Ä¶ üò≠üíî  \n",
              "16                                 –Ø –≤ —à–æ–∫–µ... üò≥ü§Ø  \n",
              "17                  ‡§ï‡•ç‡§Ø‡§æ ‡§∏‡•Ä‡§® ‡§π‡•à ‡§≠‡§æ‡§à ü§îüôÑ!! #‡§¶‡§ø‡§≤‡§ö‡§∏‡•ç‡§™  \n",
              "18                 Das ist fantastisch üòçüíØ! #Liebe  \n",
              "19                                     „Åì„Çå„ÅØ„Åô„Åî„ÅÑÔºÅüòçüëç‚ú®  \n",
              "20  Can't believe this happened... lol :D #fail üòÖ  \n",
              "21                  ¬°Esto es perfecto! üòäüíÉ #fiesta  \n",
              "22                     Ini luar biasa! üòçüëè #senang  \n",
              "23                    No me gusta para nada... üòíüôÑ  \n",
              "24                  ‡§ï‡•ç‡§Ø‡§æ ‡§∏‡•Ä‡§® ‡§π‡•à ‡§≠‡§æ‡§à ü§îüôÑ!! #‡§¶‡§ø‡§≤‡§ö‡§∏‡•ç‡§™  \n",
              "25        C‚Äôest incroyable üò≤! Tellement content üíñ  \n",
              "26                      Gak suka banget üòíüò§ #kesal  \n",
              "27                     Ini luar biasa! üòçüëè #senang  \n",
              "28                    No me gusta para nada... üòíüôÑ  \n",
              "29                     N√£o gostei nada disso :( üò°  \n",
              "30                Ich kann das nicht glauben!! üò†üò§  \n",
              "31        I love this! üòç Soooo good!!! #awesome üòä  \n",
              "32                      Gak suka banget üòíüò§ #kesal  \n",
              "33                Ich kann das nicht glauben!! üò†üò§  \n",
              "34               Je n‚Äôaime pas √ßa... :( #triste üò¢  \n",
              "35                  ¬°Esto es perfecto! üòäüíÉ #fiesta  \n",
              "36                  ¬°Esto es perfecto! üòäüíÉ #fiesta  \n",
              "37                                     „Åì„Çå„ÅØ„Åô„Åî„ÅÑÔºÅüòçüëç‚ú®  \n",
              "38                                     ‰ø°„Åò„Çâ„Çå„Å™„ÅÑ‚Ä¶ üò≠üíî  \n",
              "39                    No me gusta para nada... üòíüôÑ  \n",
              "40                      Gak suka banget üòíüò§ #kesal  \n",
              "41                                 –Ø –≤ —à–æ–∫–µ... üò≥ü§Ø  \n",
              "42  Can't believe this happened... lol :D #fail üòÖ  \n",
              "43                     Ini luar biasa! üòçüëè #senang  \n",
              "44        C‚Äôest incroyable üò≤! Tellement content üíñ  \n",
              "45               Je n‚Äôaime pas √ßa... :( #triste üò¢  \n",
              "46                  ‡§ï‡•ç‡§Ø‡§æ ‡§∏‡•Ä‡§® ‡§π‡•à ‡§≠‡§æ‡§à ü§îüôÑ!! #‡§¶‡§ø‡§≤‡§ö‡§∏‡•ç‡§™  \n",
              "47        C‚Äôest incroyable üò≤! Tellement content üíñ  \n",
              "48                                     „Åì„Çå„ÅØ„Åô„Åî„ÅÑÔºÅüòçüëç‚ú®  \n",
              "49                Ich kann das nicht glauben!! üò†üò§  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6a02cee4-a615-42e5-9e27-90c54d1003fb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>username</th>\n",
              "      <th>location</th>\n",
              "      <th>gender</th>\n",
              "      <th>age</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>user_1</td>\n",
              "      <td>India</td>\n",
              "      <td>Female</td>\n",
              "      <td>43</td>\n",
              "      <td>Can't believe this happened... lol :D #fail üòÖ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>user_2</td>\n",
              "      <td>France</td>\n",
              "      <td>Male</td>\n",
              "      <td>26</td>\n",
              "      <td>‰ø°„Åò„Çâ„Çå„Å™„ÅÑ‚Ä¶ üò≠üíî</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>user_3</td>\n",
              "      <td>Germany</td>\n",
              "      <td>Other</td>\n",
              "      <td>30</td>\n",
              "      <td>Das ist fantastisch üòçüíØ! #Liebe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>user_4</td>\n",
              "      <td>Germany</td>\n",
              "      <td>Female</td>\n",
              "      <td>38</td>\n",
              "      <td>Can't believe this happened... lol :D #fail üòÖ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>user_5</td>\n",
              "      <td>Japan</td>\n",
              "      <td>Female</td>\n",
              "      <td>39</td>\n",
              "      <td>¬°Esto es perfecto! üòäüíÉ #fiesta</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>user_6</td>\n",
              "      <td>USA</td>\n",
              "      <td>Male</td>\n",
              "      <td>45</td>\n",
              "      <td>–≠—Ç–æ –ø—Ä–æ—Å—Ç–æ —Å—É–ø–µ—Ä! üî•üî•üî•</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>user_7</td>\n",
              "      <td>France</td>\n",
              "      <td>Other</td>\n",
              "      <td>24</td>\n",
              "      <td>Ich kann das nicht glauben!! üò†üò§</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>user_8</td>\n",
              "      <td>Brazil</td>\n",
              "      <td>Female</td>\n",
              "      <td>47</td>\n",
              "      <td>Je n‚Äôaime pas √ßa... :( #triste üò¢</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>user_9</td>\n",
              "      <td>Egypt</td>\n",
              "      <td>Other</td>\n",
              "      <td>36</td>\n",
              "      <td>Ini luar biasa! üòçüëè #senang</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>user_10</td>\n",
              "      <td>USA</td>\n",
              "      <td>Male</td>\n",
              "      <td>54</td>\n",
              "      <td>Je n‚Äôaime pas √ßa... :( #triste üò¢</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>user_11</td>\n",
              "      <td>Spain</td>\n",
              "      <td>Female</td>\n",
              "      <td>38</td>\n",
              "      <td>Das ist fantastisch üòçüíØ! #Liebe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>user_12</td>\n",
              "      <td>France</td>\n",
              "      <td>Other</td>\n",
              "      <td>31</td>\n",
              "      <td>C‚Äôest incroyable üò≤! Tellement content üíñ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>user_13</td>\n",
              "      <td>Germany</td>\n",
              "      <td>Male</td>\n",
              "      <td>19</td>\n",
              "      <td>Isso √© demais cara! üòé‚úåÔ∏è #feliz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>user_14</td>\n",
              "      <td>France</td>\n",
              "      <td>Female</td>\n",
              "      <td>57</td>\n",
              "      <td>Das ist fantastisch üòçüíØ! #Liebe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>user_15</td>\n",
              "      <td>Germany</td>\n",
              "      <td>Other</td>\n",
              "      <td>34</td>\n",
              "      <td>ÿ±ÿßÿ¶ÿπ ÿ¨ÿØÿß üòÇüî•üëç</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>user_16</td>\n",
              "      <td>Japan</td>\n",
              "      <td>Male</td>\n",
              "      <td>22</td>\n",
              "      <td>‰ø°„Åò„Çâ„Çå„Å™„ÅÑ‚Ä¶ üò≠üíî</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>user_17</td>\n",
              "      <td>Indonesia</td>\n",
              "      <td>Male</td>\n",
              "      <td>44</td>\n",
              "      <td>–Ø –≤ —à–æ–∫–µ... üò≥ü§Ø</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>user_18</td>\n",
              "      <td>Brazil</td>\n",
              "      <td>Female</td>\n",
              "      <td>40</td>\n",
              "      <td>‡§ï‡•ç‡§Ø‡§æ ‡§∏‡•Ä‡§® ‡§π‡•à ‡§≠‡§æ‡§à ü§îüôÑ!! #‡§¶‡§ø‡§≤‡§ö‡§∏‡•ç‡§™</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>user_19</td>\n",
              "      <td>Germany</td>\n",
              "      <td>Female</td>\n",
              "      <td>18</td>\n",
              "      <td>Das ist fantastisch üòçüíØ! #Liebe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>user_20</td>\n",
              "      <td>Egypt</td>\n",
              "      <td>Other</td>\n",
              "      <td>34</td>\n",
              "      <td>„Åì„Çå„ÅØ„Åô„Åî„ÅÑÔºÅüòçüëç‚ú®</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>user_21</td>\n",
              "      <td>France</td>\n",
              "      <td>Male</td>\n",
              "      <td>28</td>\n",
              "      <td>Can't believe this happened... lol :D #fail üòÖ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>user_22</td>\n",
              "      <td>Brazil</td>\n",
              "      <td>Other</td>\n",
              "      <td>27</td>\n",
              "      <td>¬°Esto es perfecto! üòäüíÉ #fiesta</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>user_23</td>\n",
              "      <td>Egypt</td>\n",
              "      <td>Other</td>\n",
              "      <td>43</td>\n",
              "      <td>Ini luar biasa! üòçüëè #senang</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>user_24</td>\n",
              "      <td>France</td>\n",
              "      <td>Other</td>\n",
              "      <td>34</td>\n",
              "      <td>No me gusta para nada... üòíüôÑ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>user_25</td>\n",
              "      <td>USA</td>\n",
              "      <td>Male</td>\n",
              "      <td>46</td>\n",
              "      <td>‡§ï‡•ç‡§Ø‡§æ ‡§∏‡•Ä‡§® ‡§π‡•à ‡§≠‡§æ‡§à ü§îüôÑ!! #‡§¶‡§ø‡§≤‡§ö‡§∏‡•ç‡§™</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>user_26</td>\n",
              "      <td>Germany</td>\n",
              "      <td>Male</td>\n",
              "      <td>23</td>\n",
              "      <td>C‚Äôest incroyable üò≤! Tellement content üíñ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>user_27</td>\n",
              "      <td>USA</td>\n",
              "      <td>Other</td>\n",
              "      <td>35</td>\n",
              "      <td>Gak suka banget üòíüò§ #kesal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>user_28</td>\n",
              "      <td>Egypt</td>\n",
              "      <td>Other</td>\n",
              "      <td>30</td>\n",
              "      <td>Ini luar biasa! üòçüëè #senang</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>user_29</td>\n",
              "      <td>USA</td>\n",
              "      <td>Other</td>\n",
              "      <td>31</td>\n",
              "      <td>No me gusta para nada... üòíüôÑ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>user_30</td>\n",
              "      <td>Russia</td>\n",
              "      <td>Female</td>\n",
              "      <td>56</td>\n",
              "      <td>N√£o gostei nada disso :( üò°</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>user_31</td>\n",
              "      <td>Indonesia</td>\n",
              "      <td>Other</td>\n",
              "      <td>49</td>\n",
              "      <td>Ich kann das nicht glauben!! üò†üò§</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>user_32</td>\n",
              "      <td>Japan</td>\n",
              "      <td>Female</td>\n",
              "      <td>48</td>\n",
              "      <td>I love this! üòç Soooo good!!! #awesome üòä</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>user_33</td>\n",
              "      <td>Japan</td>\n",
              "      <td>Other</td>\n",
              "      <td>54</td>\n",
              "      <td>Gak suka banget üòíüò§ #kesal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>user_34</td>\n",
              "      <td>Brazil</td>\n",
              "      <td>Male</td>\n",
              "      <td>39</td>\n",
              "      <td>Ich kann das nicht glauben!! üò†üò§</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>user_35</td>\n",
              "      <td>Indonesia</td>\n",
              "      <td>Other</td>\n",
              "      <td>19</td>\n",
              "      <td>Je n‚Äôaime pas √ßa... :( #triste üò¢</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>user_36</td>\n",
              "      <td>India</td>\n",
              "      <td>Other</td>\n",
              "      <td>19</td>\n",
              "      <td>¬°Esto es perfecto! üòäüíÉ #fiesta</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>user_37</td>\n",
              "      <td>Germany</td>\n",
              "      <td>Male</td>\n",
              "      <td>30</td>\n",
              "      <td>¬°Esto es perfecto! üòäüíÉ #fiesta</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>user_38</td>\n",
              "      <td>Brazil</td>\n",
              "      <td>Female</td>\n",
              "      <td>33</td>\n",
              "      <td>„Åì„Çå„ÅØ„Åô„Åî„ÅÑÔºÅüòçüëç‚ú®</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>user_39</td>\n",
              "      <td>USA</td>\n",
              "      <td>Male</td>\n",
              "      <td>47</td>\n",
              "      <td>‰ø°„Åò„Çâ„Çå„Å™„ÅÑ‚Ä¶ üò≠üíî</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>user_40</td>\n",
              "      <td>Japan</td>\n",
              "      <td>Male</td>\n",
              "      <td>18</td>\n",
              "      <td>No me gusta para nada... üòíüôÑ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>user_41</td>\n",
              "      <td>Japan</td>\n",
              "      <td>Male</td>\n",
              "      <td>47</td>\n",
              "      <td>Gak suka banget üòíüò§ #kesal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>user_42</td>\n",
              "      <td>Spain</td>\n",
              "      <td>Male</td>\n",
              "      <td>29</td>\n",
              "      <td>–Ø –≤ —à–æ–∫–µ... üò≥ü§Ø</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>user_43</td>\n",
              "      <td>Germany</td>\n",
              "      <td>Female</td>\n",
              "      <td>29</td>\n",
              "      <td>Can't believe this happened... lol :D #fail üòÖ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>user_44</td>\n",
              "      <td>Japan</td>\n",
              "      <td>Other</td>\n",
              "      <td>53</td>\n",
              "      <td>Ini luar biasa! üòçüëè #senang</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>user_45</td>\n",
              "      <td>USA</td>\n",
              "      <td>Female</td>\n",
              "      <td>55</td>\n",
              "      <td>C‚Äôest incroyable üò≤! Tellement content üíñ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>user_46</td>\n",
              "      <td>USA</td>\n",
              "      <td>Male</td>\n",
              "      <td>42</td>\n",
              "      <td>Je n‚Äôaime pas √ßa... :( #triste üò¢</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>user_47</td>\n",
              "      <td>France</td>\n",
              "      <td>Other</td>\n",
              "      <td>30</td>\n",
              "      <td>‡§ï‡•ç‡§Ø‡§æ ‡§∏‡•Ä‡§® ‡§π‡•à ‡§≠‡§æ‡§à ü§îüôÑ!! #‡§¶‡§ø‡§≤‡§ö‡§∏‡•ç‡§™</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>user_48</td>\n",
              "      <td>Indonesia</td>\n",
              "      <td>Male</td>\n",
              "      <td>18</td>\n",
              "      <td>C‚Äôest incroyable üò≤! Tellement content üíñ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>user_49</td>\n",
              "      <td>Japan</td>\n",
              "      <td>Male</td>\n",
              "      <td>30</td>\n",
              "      <td>„Åì„Çå„ÅØ„Åô„Åî„ÅÑÔºÅüòçüëç‚ú®</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>user_50</td>\n",
              "      <td>Indonesia</td>\n",
              "      <td>Female</td>\n",
              "      <td>29</td>\n",
              "      <td>Ich kann das nicht glauben!! üò†üò§</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6a02cee4-a615-42e5-9e27-90c54d1003fb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6a02cee4-a615-42e5-9e27-90c54d1003fb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6a02cee4-a615-42e5-9e27-90c54d1003fb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-85352499-47eb-42c1-95ec-5525bacdde72\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-85352499-47eb-42c1-95ec-5525bacdde72')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-85352499-47eb-42c1-95ec-5525bacdde72 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"username\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"user_84\",\n          \"user_54\",\n          \"user_71\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"location\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Indonesia\",\n          \"France\",\n          \"Brazil\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gender\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Female\",\n          \"Male\",\n          \"Other\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11,\n        \"min\": 18,\n        \"max\": 60,\n        \"num_unique_values\": 37,\n        \"samples\": [\n          18,\n          34,\n          39\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tweet\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"Can't believe this happened... lol :D #fail \\ud83d\\ude05\",\n          \"I love this! \\ud83d\\ude0d Soooo good!!! #awesome \\ud83d\\ude0a\",\n          \"Gak suka banget \\ud83d\\ude12\\ud83d\\ude24 #kesal\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect spacy ftfy contractions emoji Tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEJzT9WM5kbs",
        "outputId": "d29e1cd4-cd8b-437d-c50d-7f466be17508"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting Tokenizer\n",
            "  Downloading tokenizer-3.5.1-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.16.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy) (0.2.13)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.8.3)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.3-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizer-3.5.1-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.3-py3-none-any.whl (345 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m345.1/345.1 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=67392b3004050a74ceff6a7aa7bd02174ea9c852b0f7280557dd2bf472ab5df1\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/67/88/e844b5b022812e15a52e4eaa38a1e709e99f06f6639d7e3ba7\n",
            "Successfully built langdetect\n",
            "Installing collected packages: Tokenizer, pyahocorasick, langdetect, ftfy, emoji, anyascii, textsearch, contractions\n",
            "Successfully installed Tokenizer-3.5.1 anyascii-0.3.3 contractions-0.1.73 emoji-2.14.1 ftfy-6.3.1 langdetect-1.0.9 pyahocorasick-2.2.0 textsearch-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import ftfy\n",
        "import contractions\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from langdetect import detect\n",
        "import spacy\n",
        "import emoji\n",
        "\n",
        "#Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoaNhpP46eOw",
        "outputId": "801fd709-56fc-4d8b-e7d2-807a239cd87e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Detect Language\n",
        "df['language']=df['tweet'].apply(lambda x:detect(x))\n",
        "print(df['language'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgdC06Ga7ssW",
        "outputId": "fd2d4eef-7dd8-4b66-93e0-1581f021e6d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "language\n",
            "de    14\n",
            "id    14\n",
            "fr    13\n",
            "pt    13\n",
            "ja    12\n",
            "en     8\n",
            "ru     8\n",
            "hi     7\n",
            "es     6\n",
            "ur     4\n",
            "ar     1\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Filter English Tweets\n",
        "df_en=df[df['language']=='en'].reset_index(drop=True)\n",
        "print(\"Enlish Tweets:\\n\", df_en[['tweet']].head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-_oC7xu8ZVX",
        "outputId": "77ca5ee7-7241-4e40-9a5d-4b4ca1112515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enlish Tweets:\n",
            "                                            tweet\n",
            "0  Can't believe this happened... lol :D #fail üòÖ\n",
            "1  Can't believe this happened... lol :D #fail üòÖ\n",
            "2  Can't believe this happened... lol :D #fail üòÖ\n",
            "3        I love this! üòç Soooo good!!! #awesome üòä\n",
            "4  Can't believe this happened... lol :D #fail üòÖ\n",
            "5        I love this! üòç Soooo good!!! #awesome üòä\n",
            "6        I love this! üòç Soooo good!!! #awesome üòä\n",
            "7  Can't believe this happened... lol :D #fail üòÖ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lowercasing\n",
        "df_en['tweet']=df_en['tweet'].apply(lambda x:x.lower())\n",
        "print(df_en['tweet'].head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUYPTTcm85d-",
        "outputId": "71011931-25b6-43d3-f425-c13f4f1bd368"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    can't believe this happened... lol :d #fail üòÖ\n",
            "1    can't believe this happened... lol :d #fail üòÖ\n",
            "2    can't believe this happened... lol :d #fail üòÖ\n",
            "3          i love this! üòç soooo good!!! #awesome üòä\n",
            "4    can't believe this happened... lol :d #fail üòÖ\n",
            "5          i love this! üòç soooo good!!! #awesome üòä\n",
            "6          i love this! üòç soooo good!!! #awesome üòä\n",
            "7    can't believe this happened... lol :d #fail üòÖ\n",
            "Name: tweet, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Stopword removal\n",
        "stop_words=set(stopwords.words('english'))\n",
        "\n",
        "df_en['cleaned']=df_en['tweet'].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word not in stop_words]))\n",
        "\n",
        "print(\"After Stopwords:\\n\",df_en['cleaned'].head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRNshAUK9aJB",
        "outputId": "b9435dca-ccea-4484-9a04-b3fbd4467df2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Stopwords:\n",
            " 0    ca n't believe happened ... lol : # fail üòÖ\n",
            "1    ca n't believe happened ... lol : # fail üòÖ\n",
            "2    ca n't believe happened ... lol : # fail üòÖ\n",
            "3         love ! üòç soooo good ! ! ! # awesome üòä\n",
            "4    ca n't believe happened ... lol : # fail üòÖ\n",
            "5         love ! üòç soooo good ! ! ! # awesome üòä\n",
            "6         love ! üòç soooo good ! ! ! # awesome üòä\n",
            "7    ca n't believe happened ... lol : # fail üòÖ\n",
            "Name: cleaned, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove Noise(URLs,Mentions,Hashtags,Punctuations)\n",
        "def remove_noise(text):\n",
        "  text=re.sub(r\"http\\S+|www\\S+\",\"\",text) #URLs\n",
        "  text=re.sub(r\"@\\S+\",\"\",text) #Mentions\n",
        "  text=re.sub(r\"#\\S+\",\"\",text) #Hashtags\n",
        "  text=re.sub(rf\"[{re.escape(string.punctuation)}]\",\"\",text) #Punctuations\n",
        "  return text\n",
        "\n",
        "df_en['cleaned']=df_en['cleaned'].apply(remove_noise)\n",
        "print(\"After Noise Removal:\\n\",df_en['cleaned'].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUh22TOk-HuO",
        "outputId": "42f29d39-0473-4b96-bbbe-e776343ddd22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Noise Removal:\n",
            " 0    ca nt believe happened  lol   fail üòÖ\n",
            "1    ca nt believe happened  lol   fail üòÖ\n",
            "2    ca nt believe happened  lol   fail üòÖ\n",
            "3        love  üòç soooo good     awesome üòä\n",
            "4    ca nt believe happened  lol   fail üòÖ\n",
            "Name: cleaned, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove Emojis\n",
        "def remove_emojis(text):\n",
        "  return emoji.replace_emoji(text,\"\")\n",
        "\n",
        "df_en['cleaned']=df_en['cleaned'].apply(remove_emojis)\n",
        "print(\"After Emoji Removal:\\n\",df_en['cleaned'].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbs3lv2-AC5n",
        "outputId": "07a3992d-b8d1-4b80-e668-d5ad6c0ea5fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Emoji Removal:\n",
            " 0    ca nt believe happened  lol   fail \n",
            "1    ca nt believe happened  lol   fail \n",
            "2    ca nt believe happened  lol   fail \n",
            "3         love   soooo good     awesome \n",
            "4    ca nt believe happened  lol   fail \n",
            "Name: cleaned, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Handle Contractions\n",
        "def handle_contractions(text):\n",
        "  return contractions.fix(text)\n",
        "\n",
        "df_en['cleaned']=df_en['cleaned'].apply(handle_contractions)\n",
        "print(\"After Contractions:\\n\",df_en['cleaned'].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veO4FrhdCuxq",
        "outputId": "833fea90-4ba4-45f7-c1bb-7acb861bd7c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Contractions:\n",
            " 0    ca nt believe happened  lol   fail \n",
            "1    ca nt believe happened  lol   fail \n",
            "2    ca nt believe happened  lol   fail \n",
            "3         love   soooo good     awesome \n",
            "4    ca nt believe happened  lol   fail \n",
            "Name: cleaned, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Final Processed output:\\n\",df_en[['tweet','cleaned']].head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "It_OByRRDGkU",
        "outputId": "76686534-fcfb-4597-8ea3-178b9a60c29c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Processed output:\n",
            "                                            tweet  \\\n",
            "0  can't believe this happened... lol :d #fail üòÖ   \n",
            "1  can't believe this happened... lol :d #fail üòÖ   \n",
            "2  can't believe this happened... lol :d #fail üòÖ   \n",
            "3        i love this! üòç soooo good!!! #awesome üòä   \n",
            "4  can't believe this happened... lol :d #fail üòÖ   \n",
            "5        i love this! üòç soooo good!!! #awesome üòä   \n",
            "6        i love this! üòç soooo good!!! #awesome üòä   \n",
            "7  can't believe this happened... lol :d #fail üòÖ   \n",
            "\n",
            "                               cleaned  \n",
            "0  ca nt believe happened  lol   fail   \n",
            "1  ca nt believe happened  lol   fail   \n",
            "2  ca nt believe happened  lol   fail   \n",
            "3       love   soooo good     awesome   \n",
            "4  ca nt believe happened  lol   fail   \n",
            "5       love   soooo good     awesome   \n",
            "6       love   soooo good     awesome   \n",
            "7  ca nt believe happened  lol   fail   \n"
          ]
        }
      ]
    }
  ]
}